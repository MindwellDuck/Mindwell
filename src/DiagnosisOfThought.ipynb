{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipCUMoSg6Xdu",
        "outputId": "818762ce-69d7-4f41-d3d3-0aea36a9dbaa"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'drive' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/Reddit_Data_Collection_statistics\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m all_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "folder_path = \"/content/drive/MyDrive/Reddit_Data_Collection_statistics\"\n",
        "\n",
        "all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for file_path in all_files:\n",
        "\n",
        "    df_temp = pd.read_csv(file_path)\n",
        "\n",
        "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    df_temp[\"source_file\"] = file_name\n",
        "\n",
        "    df_list.append(df_temp)\n",
        "\n",
        "df_combined = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data.csv\"\n",
        "\n",
        "df_combined.to_csv(output_path, index=False)\n",
        "print(f\"Combined data saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOHKpxfj-8w2",
        "outputId": "e28abfd2-f51e-4157-e3dc-702b0e67cfee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Reddit_Data_Collection_statistics\n"
          ]
        }
      ],
      "source": [
        "print(folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHurzSYkclSQ",
        "outputId": "7f8c317a-45a1-4847-94da-bc16907b8042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered data saved to: /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_filtered.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Provide the full path to all_data.csv in Google Drive\n",
        "csv_path = \"/content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Filter rows by text length in the 4th column\n",
        "df_filtered = df[(df.iloc[:, 4] > 10) & (df.iloc[:, 4] <= 250)]\n",
        "\n",
        "# Save output\n",
        "output_path = \"/content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_filtered.csv\"\n",
        "df_filtered.to_csv(output_path, index=False)\n",
        "print(f\"Filtered data saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHXcjzaelK7G",
        "outputId": "92506a45-c59e-4ddd-ba12-9ce76c88dc70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_1.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_2.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_3.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_4.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_5.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_6.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_7.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_8.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_9.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_10.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_11.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_12.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_13.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_14.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_15.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_16.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_17.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_18.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_19.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_20.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_21.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_22.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_23.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_24.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_25.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_26.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_27.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_28.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_29.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_30.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_31.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_32.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_33.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_34.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_35.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_36.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_37.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_38.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_39.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_40.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_41.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_42.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_43.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_44.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_45.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_46.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_47.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_48.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_49.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_50.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_51.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_52.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_53.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_54.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_55.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_56.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_57.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_58.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_59.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_60.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_61.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_62.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_63.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_64.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_65.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_66.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_67.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_68.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_69.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_70.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_71.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_72.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_73.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_74.csv with 200 rows.\n",
            "Saved /content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_75.csv with 41 rows.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# 1. Read the filtered data\n",
        "input_path = \"/content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_filtered.csv\"\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# 2. Shuffle the rows\n",
        "df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# 3. Split into multiple CSV files, each with up to 200 rows\n",
        "max_rows = 200\n",
        "total_rows = len(df_shuffled)\n",
        "\n",
        "# Calculate how many chunks are needed\n",
        "num_chunks = math.ceil(total_rows / max_rows)\n",
        "\n",
        "for i in range(num_chunks):\n",
        "    # Start and end index for the chunk\n",
        "    start_index = i * max_rows\n",
        "    end_index = min(start_index + max_rows, total_rows)\n",
        "\n",
        "    # Slice the DataFrame\n",
        "    chunk = df_shuffled.iloc[start_index:end_index]\n",
        "\n",
        "    # Name the output file (e.g., part_1, part_2, ...)\n",
        "    output_path = f\"/content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_{i+1}.csv\"\n",
        "    chunk.to_csv(output_path, index=False)\n",
        "    print(f\"Saved {output_path} with {len(chunk)} rows.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRW1qq8_5qAy",
        "outputId": "050e3523-26ef-485f-e667-7f9e11c806ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis complete. Results saved to 'analyzed_data.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "input_path = \"/content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_1.csv\"\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Define your prompts\n",
        "prompt1 = \"\"\"\n",
        "Given a text, our task is to 1) finish a few diagnose of thought questions to analyze the thought patterns of a person. Then based on the diagnose of thought analysis, 2) identify if there is cognitive distortion in the speech; 3) Recognizing the specific types of the cognitive distortion. Here we consider the following common distortions:\n",
        "\n",
        "All or Nothing Thinking/Polarized Thinking: I view a situation, a person or an event in “either-or” terms, fitting them into only two extreme categories instead of on a continuum.\n",
        "Fortune telling (also called catastrophizing): I predict the future in negative terms and believe that what will happen will be so awful that I will not be able to stand it.\n",
        "Emotional reasoning:  I believe my emotions reflect reality and let them guide my attitudes and judgments.\n",
        "Labeling/Global Labeling: I put a fixed, global label, usually negative, on myself or others.\n",
        "Mental Filter(): I pay attention to one or a few details and fail to see the whole picture.\n",
        "Mind reading: I believe that I know the thoughts or intentions of others (or that they know my thoughts or intentions) without having sufficient evidence\n",
        "Overgeneralization: I take isolated negative cases and generalize them, transforming them in a never-ending pattern, by repeatedly using words such as “always”, “never”, “ever”, “whole”, “entire”, etc\n",
        "Personalization: I assume that others’ behaviors and external events concern (or are directed to) myself without considering other plausible explanations.\n",
        "Should statements (also “musts”, “oughts”, “have tos”): I tell myself that events, people’s behaviors, and my own attitudes “should” be the way I expected them to be and not as they really are.\n",
        "Blaming (others or oneself): I direct my attention to others as sources of my negative feelings and experiences, failing to consider my own responsibility; or, conversely, I take responsibility for others’ behaviors and attitudes.\n",
        "What if?: I keep asking myself questions such as “what if something happens?”\n",
        "Discounting the positive: I disqualify positive experiences or events insisting that they do not count.\n",
        "Magnification/minimization: I evaluate myself, others, and situations placing greater importance on the negatives and/or placing much less importance on the positives.\n",
        "Jumping to conclusions (also called arbitrary inference): I draw conclusions (negative or positive) from little or no confirmatory evidence.\n",
        "Unfair comparisons: I compare myself with others who seem to do better than I do and place myself in a disadvantageous position.\n",
        "\"\"\"\n",
        "\n",
        "prompt2 = \"\"\"\n",
        "Based on this person's text, finish the following diagnosis of thought questions: 1. what is the situation? Find out the facts that are objective; what is this person thinking or imagining? Find out the thoughts or opinions that are subjective. 2. what makes this person think the thought is true or is not true? Find out the reasoning processes that support and do not support these thoughts. 3. why does this person come up with such reasoning process supporting the thought? What’s the underlying cognition mode of it?\n",
        "\"\"\"\n",
        "\n",
        "prompt3 = \"\"\"\n",
        "Please first answer: if there is cognitive distortion in the speech; Answer ’yes’ or ’no’; Please then answer: Recognizing the specific types of the cognitive distortion in the speech. There may be one type of cognitive distortion or multiple types involved. If there are multiple types, please give the top 2 dominant ones. Please only give the distortion type names separated by comma.\n",
        "\"\"\"\n",
        "\n",
        "# Function to send text to DeepSeek and get responses\n",
        "def analyze_text_with_deepseek(text):\n",
        "    # Replace with your DeepSeek API endpoint and API key\n",
        "    api_url = \"https://api.deepseek.com/v1/analyze\"\n",
        "    headers = {\n",
        "        \"Authorization\": \"\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Combine prompts with the text\n",
        "    full_prompt = f\"{prompt1}\\n\\n{text}\\n\\n{prompt2}\\n\\n{prompt3}\"\n",
        "\n",
        "    # Send request to DeepSeek\n",
        "    response = requests.post(api_url, headers=headers, json={\"prompt\": full_prompt})\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()  # Assuming DeepSeek returns a JSON response\n",
        "    else:\n",
        "        return {\"error\": f\"API request failed with status code {response.status_code}\"}\n",
        "\n",
        "# Apply the function to each row in the CSV\n",
        "df['Analysis'] = df.iloc[:, 3].apply(analyze_text_with_deepseek)\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "df.to_csv('analyzed_data.csv', index=False)\n",
        "\n",
        "print(\"Analysis complete. Results saved to 'analyzed_data.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhQjy98572gU",
        "outputId": "c78fa64b-b9e9-401b-8669-7115ffff9bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis complete. Results saved to 'analyzed_data.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize the OpenAI client with DeepSeek's API\n",
        "client = OpenAI(api_key=\"\", base_url=\"https://openrouter.ai/api/v1\")\n",
        "\n",
        "input_path = \"/content/drive/MyDrive/Reddit_Data_Collection_statistics/all_data_part_2.csv\"\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Define your prompts\n",
        "prompt1 = \"\"\"\n",
        "Given a text, our task is to 1) finish a few diagnose of thought questions to analyze the thought patterns of a person. Then based on the diagnose of thought analysis, 2) identify if there is cognitive distortion in the speech; 3) Recognizing the specific types of the cognitive distortion. Here we consider the following common distortions:\n",
        "\n",
        "All or Nothing Thinking/Polarized Thinking: I view a situation, a person or an event in “either-or” terms, fitting them into only two extreme categories instead of on a continuum.\n",
        "Fortune telling (also called catastrophizing): I predict the future in negative terms and believe that what will happen will be so awful that I will not be able to stand it.\n",
        "Emotional reasoning:  I believe my emotions reflect reality and let them guide my attitudes and judgments.\n",
        "Labeling/Global Labeling: I put a fixed, global label, usually negative, on myself or others.\n",
        "Mental Filter(): I pay attention to one or a few details and fail to see the whole picture.\n",
        "Mind reading: I believe that I know the thoughts or intentions of others (or that they know my thoughts or intentions) without having sufficient evidence\n",
        "Overgeneralization: I take isolated negative cases and generalize them, transforming them in a never-ending pattern, by repeatedly using words such as “always”, “never”, “ever”, “whole”, “entire”, etc\n",
        "Personalization: I assume that others’ behaviors and external events concern (or are directed to) myself without considering other plausible explanations.\n",
        "Should statements (also “musts”, “oughts”, “have tos”): I tell myself that events, people’s behaviors, and my own attitudes “should” be the way I expected them to be and not as they really are.\n",
        "Blaming (others or oneself): I direct my attention to others as sources of my negative feelings and experiences, failing to consider my own responsibility; or, conversely, I take responsibility for others’ behaviors and attitudes.\n",
        "What if?: I keep asking myself questions such as “what if something happens?”\n",
        "Discounting the positive: I disqualify positive experiences or events insisting that they do not count.\n",
        "Magnification/minimization: I evaluate myself, others, and situations placing greater importance on the negatives and/or placing much less importance on the positives.\n",
        "Jumping to conclusions (also called arbitrary inference): I draw conclusions (negative or positive) from little or no confirmatory evidence.\n",
        "Unfair comparisons: I compare myself with others who seem to do better than I do and place myself in a disadvantageous position.\n",
        "\"\"\"\n",
        "\n",
        "prompt2 = \"\"\"\n",
        "Based on this person's text, finish the following diagnosis of thought questions: 1. what is the situation? Find out the facts that are objective; what is this person thinking or imagining? Find out the thoughts or opinions that are subjective. 2. what makes this person think the thought is true or is not true? Find out the reasoning processes that support and do not support these thoughts. 3. why does this person come up with such reasoning process supporting the thought? What’s the underlying cognition mode of it?\n",
        "\"\"\"\n",
        "\n",
        "prompt3 = \"\"\"\n",
        "Please first answer: if there is cognitive distortion in the speech; Answer ’yes’ or ’no’; Please then answer: Recognizing the specific types of the cognitive distortion in the speech. There may be one type of cognitive distortion or multiple types involved. If there are multiple types, please give the top 2 dominant ones. Please only give the distortion type names separated by comma.\n",
        "\"\"\"\n",
        "\n",
        "# Function to send text to DeepSeek and get responses\n",
        "def analyze_text_with_deepseek(text):\n",
        "    # Combine prompts with the text\n",
        "    full_prompt = f\"{prompt1}\\n\\n{text}\\n\\n{prompt2}\\n\\n{prompt3}\"\n",
        "\n",
        "    # Send request to DeepSeek\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"deepseek/deepseek-r1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": full_prompt},\n",
        "        ],\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    # Extract the response content\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Apply the function to the fourth column (index 3) in each row\n",
        "df['Analysis'] = df.iloc[:, 3].apply(analyze_text_with_deepseek)\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "df.to_csv('analyzed_data.csv', index=False)\n",
        "\n",
        "print(\"Analysis complete. Results saved to 'analyzed_data.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 1/200 rows (0.50%)\n",
            "Processed 2/200 rows (1.00%)\n",
            "Processed 3/200 rows (1.50%)\n",
            "Processed 4/200 rows (2.00%)\n",
            "Processed 5/200 rows (2.50%)\n",
            "Processed 6/200 rows (3.00%)\n",
            "Processed 7/200 rows (3.50%)\n",
            "Processed 8/200 rows (4.00%)\n",
            "Processed 9/200 rows (4.50%)\n",
            "Processed 10/200 rows (5.00%)\n",
            "Processed 11/200 rows (5.50%)\n",
            "Processed 12/200 rows (6.00%)\n",
            "Processed 13/200 rows (6.50%)\n",
            "Processed 14/200 rows (7.00%)\n",
            "Processed 15/200 rows (7.50%)\n",
            "Processed 16/200 rows (8.00%)\n",
            "Processed 17/200 rows (8.50%)\n",
            "Processed 18/200 rows (9.00%)\n",
            "Processed 19/200 rows (9.50%)\n",
            "Processed 20/200 rows (10.00%)\n",
            "Processed 21/200 rows (10.50%)\n",
            "Processed 22/200 rows (11.00%)\n",
            "Processed 23/200 rows (11.50%)\n",
            "Processed 24/200 rows (12.00%)\n",
            "Processed 25/200 rows (12.50%)\n",
            "Processed 26/200 rows (13.00%)\n",
            "Processed 27/200 rows (13.50%)\n",
            "Processed 28/200 rows (14.00%)\n",
            "Processed 29/200 rows (14.50%)\n",
            "Processed 30/200 rows (15.00%)\n",
            "Processed 31/200 rows (15.50%)\n",
            "Processed 32/200 rows (16.00%)\n",
            "Processed 33/200 rows (16.50%)\n",
            "Processed 34/200 rows (17.00%)\n",
            "Processed 35/200 rows (17.50%)\n",
            "Processed 36/200 rows (18.00%)\n",
            "Processed 37/200 rows (18.50%)\n",
            "Processed 38/200 rows (19.00%)\n",
            "Processed 39/200 rows (19.50%)\n",
            "Processed 40/200 rows (20.00%)\n",
            "Processed 41/200 rows (20.50%)\n",
            "Processed 42/200 rows (21.00%)\n",
            "Processed 43/200 rows (21.50%)\n",
            "Processed 44/200 rows (22.00%)\n",
            "Processed 45/200 rows (22.50%)\n",
            "Processed 46/200 rows (23.00%)\n",
            "Processed 47/200 rows (23.50%)\n",
            "Processed 48/200 rows (24.00%)\n",
            "Processed 49/200 rows (24.50%)\n",
            "Processed 50/200 rows (25.00%)\n",
            "Processed 51/200 rows (25.50%)\n",
            "Processed 52/200 rows (26.00%)\n",
            "Processed 53/200 rows (26.50%)\n",
            "Processed 54/200 rows (27.00%)\n",
            "Processed 55/200 rows (27.50%)\n",
            "Processed 56/200 rows (28.00%)\n",
            "Processed 57/200 rows (28.50%)\n",
            "Processed 58/200 rows (29.00%)\n",
            "Processed 59/200 rows (29.50%)\n",
            "Processed 60/200 rows (30.00%)\n",
            "Processed 61/200 rows (30.50%)\n",
            "Processed 62/200 rows (31.00%)\n",
            "Processed 63/200 rows (31.50%)\n",
            "Processed 64/200 rows (32.00%)\n",
            "Processed 65/200 rows (32.50%)\n",
            "Processed 66/200 rows (33.00%)\n",
            "Processed 67/200 rows (33.50%)\n",
            "Processed 68/200 rows (34.00%)\n",
            "Processed 69/200 rows (34.50%)\n",
            "Processed 70/200 rows (35.00%)\n",
            "Processed 71/200 rows (35.50%)\n",
            "Processed 72/200 rows (36.00%)\n",
            "Processed 73/200 rows (36.50%)\n",
            "Processed 74/200 rows (37.00%)\n",
            "Processed 75/200 rows (37.50%)\n",
            "Processed 76/200 rows (38.00%)\n",
            "Processed 77/200 rows (38.50%)\n",
            "Processed 78/200 rows (39.00%)\n",
            "Processed 79/200 rows (39.50%)\n",
            "Processed 80/200 rows (40.00%)\n",
            "Processed 81/200 rows (40.50%)\n",
            "Processed 82/200 rows (41.00%)\n",
            "Processed 83/200 rows (41.50%)\n",
            "Processed 84/200 rows (42.00%)\n",
            "Processed 85/200 rows (42.50%)\n",
            "Processed 86/200 rows (43.00%)\n",
            "Processed 87/200 rows (43.50%)\n",
            "Processed 88/200 rows (44.00%)\n",
            "Processed 89/200 rows (44.50%)\n",
            "Processed 90/200 rows (45.00%)\n",
            "Processed 91/200 rows (45.50%)\n",
            "Processed 92/200 rows (46.00%)\n",
            "Processed 93/200 rows (46.50%)\n",
            "Processed 94/200 rows (47.00%)\n",
            "Processed 95/200 rows (47.50%)\n",
            "Processed 96/200 rows (48.00%)\n",
            "Processed 97/200 rows (48.50%)\n",
            "Processed 98/200 rows (49.00%)\n",
            "Processed 99/200 rows (49.50%)\n",
            "Processed 100/200 rows (50.00%)\n",
            "Processed 101/200 rows (50.50%)\n",
            "Processed 102/200 rows (51.00%)\n",
            "Processed 103/200 rows (51.50%)\n",
            "Processed 104/200 rows (52.00%)\n",
            "Processed 105/200 rows (52.50%)\n",
            "Processed 106/200 rows (53.00%)\n",
            "Processed 107/200 rows (53.50%)\n",
            "Processed 108/200 rows (54.00%)\n",
            "Processed 109/200 rows (54.50%)\n",
            "Processed 110/200 rows (55.00%)\n",
            "Processed 111/200 rows (55.50%)\n",
            "Processed 112/200 rows (56.00%)\n",
            "Processed 113/200 rows (56.50%)\n",
            "Processed 114/200 rows (57.00%)\n",
            "Processed 115/200 rows (57.50%)\n",
            "Processed 116/200 rows (58.00%)\n",
            "Processed 117/200 rows (58.50%)\n",
            "Processed 118/200 rows (59.00%)\n",
            "Processed 119/200 rows (59.50%)\n",
            "Processed 120/200 rows (60.00%)\n",
            "Processed 121/200 rows (60.50%)\n",
            "Processed 122/200 rows (61.00%)\n",
            "Processed 123/200 rows (61.50%)\n",
            "Processed 124/200 rows (62.00%)\n",
            "Processed 125/200 rows (62.50%)\n",
            "Processed 126/200 rows (63.00%)\n",
            "Processed 127/200 rows (63.50%)\n",
            "Processed 128/200 rows (64.00%)\n",
            "Processed 129/200 rows (64.50%)\n",
            "Processed 130/200 rows (65.00%)\n",
            "Processed 131/200 rows (65.50%)\n",
            "Processed 132/200 rows (66.00%)\n",
            "Processed 133/200 rows (66.50%)\n",
            "Processed 134/200 rows (67.00%)\n",
            "Processed 135/200 rows (67.50%)\n",
            "Processed 136/200 rows (68.00%)\n",
            "Processed 137/200 rows (68.50%)\n",
            "Processed 138/200 rows (69.00%)\n",
            "Processed 139/200 rows (69.50%)\n",
            "Processed 140/200 rows (70.00%)\n",
            "Processed 141/200 rows (70.50%)\n",
            "Processed 142/200 rows (71.00%)\n",
            "Processed 143/200 rows (71.50%)\n",
            "Processed 144/200 rows (72.00%)\n",
            "Processed 145/200 rows (72.50%)\n",
            "Processed 146/200 rows (73.00%)\n",
            "Processed 147/200 rows (73.50%)\n",
            "Processed 148/200 rows (74.00%)\n",
            "Processed 149/200 rows (74.50%)\n",
            "Processed 150/200 rows (75.00%)\n",
            "Processed 151/200 rows (75.50%)\n",
            "Processed 152/200 rows (76.00%)\n",
            "Processed 153/200 rows (76.50%)\n",
            "Processed 154/200 rows (77.00%)\n",
            "Processed 155/200 rows (77.50%)\n",
            "Processed 156/200 rows (78.00%)\n",
            "Processed 157/200 rows (78.50%)\n",
            "Processed 158/200 rows (79.00%)\n",
            "Processed 159/200 rows (79.50%)\n",
            "Processed 160/200 rows (80.00%)\n",
            "Processed 161/200 rows (80.50%)\n",
            "Processed 162/200 rows (81.00%)\n",
            "Processed 163/200 rows (81.50%)\n",
            "Processed 164/200 rows (82.00%)\n",
            "Processed 165/200 rows (82.50%)\n",
            "Processed 166/200 rows (83.00%)\n",
            "Processed 167/200 rows (83.50%)\n",
            "Processed 168/200 rows (84.00%)\n",
            "Processed 169/200 rows (84.50%)\n",
            "Processed 170/200 rows (85.00%)\n",
            "Processed 171/200 rows (85.50%)\n",
            "Processed 172/200 rows (86.00%)\n",
            "Processed 173/200 rows (86.50%)\n",
            "Processed 174/200 rows (87.00%)\n",
            "Processed 175/200 rows (87.50%)\n",
            "Processed 176/200 rows (88.00%)\n",
            "Processed 177/200 rows (88.50%)\n",
            "Processed 178/200 rows (89.00%)\n",
            "Processed 179/200 rows (89.50%)\n",
            "Processed 180/200 rows (90.00%)\n",
            "Processed 181/200 rows (90.50%)\n",
            "Processed 182/200 rows (91.00%)\n",
            "Processed 183/200 rows (91.50%)\n",
            "Processed 184/200 rows (92.00%)\n",
            "Processed 185/200 rows (92.50%)\n",
            "Processed 186/200 rows (93.00%)\n",
            "Processed 187/200 rows (93.50%)\n",
            "Processed 188/200 rows (94.00%)\n",
            "Processed 189/200 rows (94.50%)\n",
            "Processed 190/200 rows (95.00%)\n",
            "Processed 191/200 rows (95.50%)\n",
            "Processed 192/200 rows (96.00%)\n",
            "Processed 193/200 rows (96.50%)\n",
            "Processed 194/200 rows (97.00%)\n",
            "Processed 195/200 rows (97.50%)\n",
            "Processed 196/200 rows (98.00%)\n",
            "Processed 197/200 rows (98.50%)\n",
            "Processed 198/200 rows (99.00%)\n",
            "Processed 199/200 rows (99.50%)\n",
            "Processed 200/200 rows (100.00%)\n",
            "Analysis complete. Results saved to 'analyzed_data.csv'.\n"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "input_path = \"~/Downloads/all_data_part_3.csv\"  # Adjust path as needed\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Define your prompts\n",
        "prompt1 = \"\"\"\n",
        "Given a text, our task is to 1) finish a few diagnose of thought questions to analyze the thought patterns of a person. Then based on the diagnose of thought analysis, 2) identify if there is cognitive distortion in the speech; 3) Recognizing the specific types of the cognitive distortion. Here we consider the following common distortions:\n",
        "\n",
        "All or Nothing Thinking/Polarized Thinking: I view a situation, a person or an event in “either-or” terms, fitting them into only two extreme categories instead of on a continuum.\n",
        "Fortune telling (also called catastrophizing): I predict the future in negative terms and believe that what will happen will be so awful that I will not be able to stand it.\n",
        "Emotional reasoning:  I believe my emotions reflect reality and let them guide my attitudes and judgments.\n",
        "Labeling/Global Labeling: I put a fixed, global label, usually negative, on myself or others.\n",
        "Mental Filter(): I pay attention to one or a few details and fail to see the whole picture.\n",
        "Mind reading: I believe that I know the thoughts or intentions of others (or that they know my thoughts or intentions) without having sufficient evidence\n",
        "Overgeneralization: I take isolated negative cases and generalize them, transforming them in a never-ending pattern, by repeatedly using words such as “always”, “never”, “ever”, “whole”, “entire”, etc\n",
        "Personalization: I assume that others’ behaviors and external events concern (or are directed to) myself without considering other plausible explanations.\n",
        "Should statements (also “musts”, “oughts”, “have tos”): I tell myself that events, people’s behaviors, and my own attitudes “should” be the way I expected them to be and not as they really are.\n",
        "Blaming (others or oneself): I direct my attention to others as sources of my negative feelings and experiences, failing to consider my own responsibility; or, conversely, I take responsibility for others’ behaviors and attitudes.\n",
        "What if?: I keep asking myself questions such as “what if something happens?”\n",
        "Discounting the positive: I disqualify positive experiences or events insisting that they do not count.\n",
        "Magnification/minimization: I evaluate myself, others, and situations placing greater importance on the negatives and/or placing much less importance on the positives.\n",
        "Jumping to conclusions (also called arbitrary inference): I draw conclusions (negative or positive) from little or no confirmatory evidence.\n",
        "Unfair comparisons: I compare myself with others who seem to do better than I do and place myself in a disadvantageous position.\n",
        "\"\"\"\n",
        "\n",
        "prompt2 = \"\"\"\n",
        "Based on this person's text, finish the following diagnosis of thought questions: 1. what is the situation? Find out the facts that are objective; what is this person thinking or imagining? Find out the thoughts or opinions that are subjective. 2. what makes this person think the thought is true or is not true? Find out the reasoning processes that support and do not support these thoughts. 3. why does this person come up with such reasoning process supporting the thought? What’s the underlying cognition mode of it?\n",
        "\"\"\"\n",
        "\n",
        "prompt3 = \"\"\"\n",
        "Please first answer: if there is cognitive distortion in the speech; Answer ’yes’ or ’no’; Please then answer: Recognizing the specific types of the cognitive distortion in the speech. There may be one type of cognitive distortion or multiple types involved. If there are multiple types, please give the top 2 dominant ones. Please only give the distortion type names separated by comma.\n",
        "\"\"\"\n",
        "\n",
        "def analyze_text_with_ollama(text):\n",
        "    full_prompt = f\"{prompt1}\\n\\n{text}\\n\\n{prompt2}\\n\\n{prompt3}\"\n",
        "    \n",
        "    response = ollama.chat(model=\"deepseek-r1:14b\", messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": full_prompt}\n",
        "    ])\n",
        "    \n",
        "    return response['message']['content']\n",
        "\n",
        "# Apply function to dataset with progress tracking\n",
        "total_rows = len(df)\n",
        "for i, row in enumerate(df.iterrows(), start=1):\n",
        "    text = row[1].iloc[3]  # Assuming text is in the fourth column\n",
        "    df.at[row[0], 'Analysis'] = analyze_text_with_ollama(text)\n",
        "    \n",
        "    # Print progress\n",
        "    print(f\"Processed {i}/{total_rows} rows ({(i/total_rows)*100:.2f}%)\", flush=True)\n",
        "\n",
        "# Save results\n",
        "df.to_csv('../data/analyzed_data.csv', index=False)\n",
        "print(\"Analysis complete. Results saved to 'analyzed_data.csv'.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
