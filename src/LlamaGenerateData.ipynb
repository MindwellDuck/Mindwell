{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama_model():\n",
    "    \"\"\"Load LLaMA model and tokenizer\"\"\"\n",
    "    model_name = \"/home/g4/Llama-3.2-3B-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "model, tokenizer, device = load_llama_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cognitive_distortions = {\n",
    "    \"All or Nothing Thinking\": \"I view a situation, a person or an event in “either-or” terms, fitting them into only two extreme categories instead of on a continuum.\",\n",
    "    \"Fortune Telling\": \"I predict the future in negative terms and believe that what will happen will be so awful that I will not be able to stand it.\",\n",
    "    \"Emotional Reasoning\": \"I believe my emotions reflect reality and let them guide my attitudes and judgments.\",\n",
    "    \"Labeling/Global Labeling\": \"I put a fixed, usually negative, global label on myself or others.\",\n",
    "    \"Mental Filter\": \"I pay attention to one or a few details and fail to see the whole picture.\",\n",
    "    \"Mind Reading\": \"I believe that I know the thoughts or intentions of others (or that they know my thoughts or intentions) without sufficient evidence.\",\n",
    "    \"Overgeneralization\": \"I take isolated negative cases and generalize them, using words like “always,” “never,” “whole,” “entire,” etc.\",\n",
    "    \"Personalization\": \"I assume that others’ behaviors and external events concern myself without considering other plausible explanations.\",\n",
    "    \"Should Statements\": \"I tell myself that events, people’s behaviors, and my own attitudes “should” be the way I expected, not as they are.\",\n",
    "    \"Blaming\": \"I direct blame to others for my negative feelings or take responsibility for others' behaviors and attitudes.\",\n",
    "    \"What if?\": \"I keep asking questions like “what if something happens?” focusing on negative outcomes.\",\n",
    "    \"Discounting the Positive\": \"I disqualify positive experiences or events, insisting that they don’t count.\",\n",
    "    \"Magnification/Minimization\": \"I emphasize the negatives or downplay positives in myself, others, or situations.\",\n",
    "    \"Jumping to Conclusions\": \"I draw conclusions from little or no confirmatory evidence.\",\n",
    "    \"Unfair Comparisons\": \"I compare myself with others who seem better and place myself at a disadvantage.\"\n",
    "}\n",
    "\n",
    "# Variants of tone, style, intensity, and other characteristics for diverse responses\n",
    "tones = [\"angry\", \"calm\", \"frustrated\", \"sarcastic\", \"venting\", \"blunt\", \"sad\", \"anxious\", \"direct\"]\n",
    "structures = [\"structured\", \"scattered\", \"nuanced\", \"blunt\", \"sarcastic\"]\n",
    "intensities = [\"mild\", \"intense\", \"slightly annoyed\", \"highly emotional\", \"casual\"]\n",
    "use_of_language = [\"use cuss words\", \"avoid cuss words\", \"use formal language\", \"use casual language\"]\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are an expert assistant specializing in identifying and generating text examples that reflect common cognitive distortions.\n",
    "Your task is to generate writing prompts and then create text from the perspective of people experiencing specific cognitive distortions without being aware of it.\n",
    "Each time you are asked to generate text, remember that the person should not be aware of their cognitive distortion.\n",
    "Make sure the situation, writing style, and personality of the person are different for each response.\n",
    "The final text should demonstrate the cognitive distortion in a subtle, human, realistic way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(text: str, count) -> str:\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    " ]\n",
    "    inputs = tokenizer.apply_chat_template(messages, tokenize=True, return_dict=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=count,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(cognitive_distortion):\n",
    "    # Look up the definition for the specific cognitive distortion\n",
    "    definition = cognitive_distortions[cognitive_distortion]\n",
    "    \n",
    "    # Randomly choose characteristics for varied human-like writing styles\n",
    "    chosen_tone = random.choice(tones)\n",
    "    chosen_structure = random.choice(structures)\n",
    "    chosen_intensity = random.choice(intensities)\n",
    "    chosen_language = random.choice(use_of_language)\n",
    "\n",
    "    user_message = f\"Generate a writing prompt for the cognitive distortion '{cognitive_distortion}'. The definition of this cognitive distortion is: '{definition}'. Write as if the person experiencing this distortion has a {chosen_tone} tone, with a {chosen_structure} structure, expressing a {chosen_intensity} level of intensity, and {chosen_language}. The prompt should ask Llama 3.2 to write from the perspective of someone experiencing this distortion, and the person should not be aware of it. DO NOT PROVIDE AN EXAMPLE! ONLY RESPOND WITH THE PROMPT, DO NOT ADD ANY EXTRA CONTEXT!\"\n",
    "    \n",
    "    return generate_response(user_message, 1000)\n",
    "\n",
    "\n",
    "def generate_text_from_prompt(prompt, cognitive_distortion):\n",
    "    user_message = f\"Using the following writing prompt, generate one example from the perspective of someone experiencing the cognitive distortion '{cognitive_distortion}'. Make sure they are not aware of the distortion, and the text should demonstrate their behavior subtly. Prompt: {prompt} RESPOND WITH ONLY THE EXAMPLE, NO EXTRA CONTEXT!\"\n",
    "\n",
    "    return generate_response(user_message, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = \"{desc:<30}{percentage:3.0f}%|{bar}{r_bar}\"\n",
    "for i in trange(1000, leave=False, desc='Iteration', ascii=True, bar_format=format): # run 100 times, resulting in 2000 samples for each distortion\n",
    "    data = []\n",
    "    for distortion, definition in tqdm(cognitive_distortions.items(), desc=\"Distortions\", leave=False, bar_format=format):\n",
    "        for _ in trange(5, desc=distortion, leave=False, bar_format=format):  # 5 samples for each distortion\n",
    "            try:\n",
    "                generated_prompt = generate_prompt(distortion).removesuffix('<|eot_id|>')     \n",
    "                generated_text = generate_text_from_prompt(generated_prompt, distortion).removesuffix('<|eot_id|>')\n",
    "                \n",
    "                data.append({\n",
    "                    \"cognitive_distortion\": distortion,  # label the cognitive distortion\n",
    "                    \"generated_prompt\": generated_prompt,\n",
    "                    \"generated_text\": generated_text\n",
    "                })                \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating text for {distortion}: {e}\")\n",
    "\n",
    "    with open(f'/home/g4/Mindwell/data/{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}.json', 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
